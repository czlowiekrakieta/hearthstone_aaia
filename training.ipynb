{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "READING:  p1_vecs.npz\n",
      "READING:  p2_vecs.npz\n",
      "0\n",
      "BUILDING SPLIT DATASET COMPLETED\n",
      "INPUT SHAPE OF ONE SIAMESE LEG:  332\n",
      "Train on 146273 samples, validate on 27226 samples\n",
      "Epoch 1/1\n",
      " 62944/146273 [===========>..................] - ETA: 16s - loss: 0.6816"
     ]
    }
   ],
   "source": [
    "from hearth.backend.cross_validate import load_and_split\n",
    "from hearth.backend.utils import build_full_cards_and_heroes_list, vectorize_deck, get_card_idx_map\n",
    "from hearth.backend.config import TRAINING_GAMES, DECKS_JSON\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from os.path import exists\n",
    "\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "player_one_file = 'p1_vecs.npz'\n",
    "player_two_file = 'p2_vecs.npz'\n",
    "model_type = 'siamese'\n",
    "fraction = 0.25\n",
    "layers = [200, 200]\n",
    "dropout = [.5, .5]\n",
    "# def main(model_type='gbc', layers=None, dropout=None,\n",
    "#          player_one_file=None, player_two_file=None, fraction=0.25):\n",
    "decks_dicts = []\n",
    "\n",
    "if not 0 < fraction <= 1.:\n",
    "    raise ValueError(\"fraction has to be between 0 and 1\")\n",
    "\n",
    "f = open(DECKS_JSON, \"r\")\n",
    "\n",
    "for r in f.readlines():\n",
    "    decks_dicts.append(json.loads(r))\n",
    "\n",
    "decks_named_dicts = {x['deckName'][0]: x for x in decks_dicts}\n",
    "\n",
    "def include_bot(bot_deck_str):\n",
    "    bot, deck = bot_deck_str.split('_')\n",
    "    z = {'bot': bot}\n",
    "    z.update(decks_named_dicts[deck])\n",
    "    return z\n",
    "\n",
    "cards, heroes = build_full_cards_and_heroes_list(DECKS_JSON)\n",
    "card_map = get_card_idx_map(cards)\n",
    "hero_map = get_card_idx_map(heroes)  # misleading name\n",
    "#     decks_named_dicts =\n",
    "\n",
    "# loads battle file\n",
    "battles_df = pd.read_csv(TRAINING_GAMES, sep=\";\", header=None)\n",
    "if not exists(player_one_file):\n",
    "    battles_df['bot_with_deck_p1'] = battles_df[1] + '_' + battles_df[2]\n",
    "    p1_arr = battles_df['bot_with_deck_p1'].apply(include_bot).apply(vectorize_deck,\n",
    "                                                                     args=(card_map, hero_map),\n",
    "                                                                     one_hot=False)\n",
    "    p1_arr = np.vstack(p1_arr)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    p1_arr = csr_matrix(p1_arr)\n",
    "    save_npz(player_one_file, p1_arr)\n",
    "else:\n",
    "    print(\"READING: \", player_one_file)\n",
    "    p1_arr = load_npz(player_one_file)\n",
    "\n",
    "if not exists(player_two_file):\n",
    "    battles_df['bot_with_deck_p2'] = battles_df[3] + '_' + battles_df[4]\n",
    "    p2_arr = battles_df['bot_with_deck_p2'].apply(include_bot).apply(vectorize_deck,\n",
    "                                                                     args=(card_map, hero_map),\n",
    "                                                                     one_hot=False)\n",
    "    p2_arr = np.vstack(p2_arr)\n",
    "    p2_arr = csr_matrix(p2_arr)\n",
    "    save_npz(player_two_file, p2_arr)\n",
    "else:\n",
    "    print(\"READING: \", player_two_file)\n",
    "    p2_arr = load_npz(player_one_file)\n",
    "\n",
    "p1_arr, p2_arr, y = np.array(p1_arr.todense()), np.array(p2_arr.todense()), battles_df[5]\n",
    "y = (y == 'PLAYER_1 WON').astype(int)\n",
    "y = y.as_matrix()\n",
    "\n",
    "if fraction < 1:\n",
    "    idx = np.random.permutation(p1_arr.shape[0])\n",
    "    p1_arr = p1_arr[idx, :]\n",
    "    p2_arr = p2_arr[idx, :]\n",
    "    y = y[idx]\n",
    "\n",
    "probas = []\n",
    "ys = []\n",
    "\n",
    "for n in range(N_SPLITS):\n",
    "    print(n)\n",
    "    train, test = load_and_split()[1:]\n",
    "\n",
    "    if model_type == 'gbc':\n",
    "        p1_train = p1_arr[train]\n",
    "        p2_train = p2_arr[train]\n",
    "\n",
    "        p1_test = p1_arr[test]\n",
    "        p2_test = p2_arr[test]\n",
    "\n",
    "        X_train_ord = np.hstack((p1_train, p2_train))\n",
    "        X_train_rev = np.hstack((p2_train, p1_train))\n",
    "\n",
    "        X_test_ord = np.hstack((p1_test, p2_test))\n",
    "        X_test_rev = np.hstack((p2_test, p1_test))\n",
    "\n",
    "        y_train = y[train]\n",
    "        y_train = np.hstack((y_train, 1-y_train))\n",
    "\n",
    "        y_test = y[test]\n",
    "        y_test = np.hstack((y_test, 1-y_test))\n",
    "\n",
    "        X_train = np.vstack((X_train_ord, X_train_rev))\n",
    "        X_test = np.vstack((X_test_ord, X_test_rev))\n",
    "        model = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "    elif model_type == 'siamese':\n",
    "        from hearth.backend.siamese import build_siam\n",
    "        from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        \n",
    "        p1_train = p1_arr[train]\n",
    "        p2_train = p2_arr[train]\n",
    "\n",
    "        p1_test = p1_arr[test]\n",
    "        p2_test = p2_arr[test]\n",
    "        \n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        p = p1_train.shape[1]\n",
    "        print('INPUT SHAPE OF ONE SIAMESE LEG: ', p)\n",
    "        model = build_siam(layers=layers, dropout=dropout, input_shape=p)\n",
    "        model.compile('SGD', 'binary_crossentropy')\n",
    "        X_test = [p1_test, p2_test]\n",
    "\n",
    "        model.fit([p1_train, p2_train], y_train, validation_data=(X_test, y_test))\n",
    "    else:\n",
    "        raise NameError(\"model_type should be either gbc or siamese\")\n",
    "\n",
    "    probas.append(model.predict(X_test))\n",
    "    ys.append(y_test)\n",
    "\n",
    "#     return probas, ys\n",
    "\n",
    "#     for n in range(N_SPLITS):\n",
    "#         tr = all_decks[offset:offset+fold_size]\n",
    "#         offset += fold_size\n",
    "\n",
    "#         df, train, test = load_and_split()\n",
    "\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model_type', type=str, default='siamese')\n",
    "# parser.add_argument('--layers', type=int, nargs='+', default=[100, 200])\n",
    "# parser.add_argument('--dropout', type=float, nargs='+', default=[.1, .1])\n",
    "# parser.add_argument('--player_one_file', type=str, required=True)\n",
    "# parser.add_argument('--player_two_file', type=str, required=True)\n",
    "# parser.add_argument('--fraction', type=float, default=0.25)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     z = '--model_type siamese --player_one_file p1_vecs.npz --player_two_file p2_vecs.npz'\n",
    "#     probas, ys = main(**vars(parser.parse_known_args(z.split())[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
